{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c7cadc-e040-4532-8aa1-681c40de28d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT: /home/cis6022/Adam/varMax\n",
      "PKG_DIR: /home/cis6022/StarLiteGAN/packages\n",
      "DATA_DIR: /home/cis6022/Adam/varMax/data\n",
      "device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cis6022/StarLiteGAN/packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys, csv, random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np, torch, torch.nn as nn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "ROOT = os.getcwd()\n",
    "# PKG_DIR = \"/home/cis6022/StarLiteGAN/packages\"\n",
    "# if PKG_DIR not in sys.path: sys.path.insert(0, PKG_DIR)\n",
    "DATA_DIR = Path(ROOT) / \"data\"; DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "seed = 1337\n",
    "random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\"); device_type = \"cuda\" if use_cuda else \"cpu\"\n",
    "if use_cuda: torch.cuda.manual_seed_all(seed); torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(\"ROOT:\", ROOT); print(\"device:\", device); # print(\"PKG_DIR:\", PKG_DIR); print(\"DATA_DIR:\", str(DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b1b5cf4f-bd55-45e3-b1ce-debd86a471d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: /home/cis6022/Adam/varMax/data/CICIDS-2017_preprocessed.csv\n",
      "Loading: /home/cis6022/Adam/varMax/data/CICIDS_live_capture_preprocessed2.csv\n",
      "[warn] live benign short: have 13970 need 181053 -> oversampling 167083 rows\n",
      "X shape: (987177, 1, 1504) num_features: 1504\n",
      "splits: 987177 282050 141028\n",
      "benign/mal (MAIN original): 362108 1048147\n",
      "benign live available: 13970 | live used total: 181053\n",
      "benign live per split train/val/test: 126737 36210 18106\n",
      "pos rate train/val/test: 0.7432324886322021 0.7432334423065186 0.7432283163070679\n",
      "pos_weight_value (n_neg/n_pos): 0.3454740345478058\n",
      "feature_names: 1504 | scaler_mean/scale: 1504 1504\n"
     ]
    }
   ],
   "source": [
    "# --- DATA LOAD + PREP (mix benign 50/50 main vs live across ALL splits) ---\n",
    "\n",
    "MAIN_CSV=\"CICIDS-2017_preprocessed.csv\"\n",
    "LIVE_CSV=\"CICIDS_live_capture_preprocessed2.csv\"\n",
    "\n",
    "def _read_preprocessed(name, feature_names=None):\n",
    "    p=(DATA_DIR/name) if (DATA_DIR/name).exists() else (Path(ROOT)/name)\n",
    "    print(\"Loading:\", p)\n",
    "    df=pd.read_csv(p)\n",
    "    if \"label\" in df.columns and \"Label\" not in df.columns: df=df.rename(columns={\"label\":\"Label\"})\n",
    "    if \"Label\" not in df.columns: raise SystemExit(f\"{name} missing Label column. Columns: {list(df.columns)[:20]} ...\")\n",
    "\n",
    "    df=df.replace([np.inf,-np.inf],np.nan).dropna()\n",
    "\n",
    "    # drop CICIDS meta (not features)\n",
    "    for c in (\"Flow ID\",\"Source IP\",\"Source Port\",\"Destination IP\",\"Destination Port\",\"Timestamp\"):\n",
    "        if c in df.columns: df=df.drop(columns=c)\n",
    "\n",
    "    labels=df[\"Label\"].astype(str).str.strip()\n",
    "    y=(labels.str.upper()!=\"BENIGN\").astype(np.int64).to_numpy()  # 0=benign, 1=malicious\n",
    "\n",
    "    X_df=df.drop(columns=[\"Label\"]).select_dtypes(include=[np.number])\n",
    "\n",
    "    # enforce exact same columns/order as training feature_names\n",
    "    if feature_names is not None:\n",
    "        for c in feature_names:\n",
    "            if c not in X_df.columns: X_df[c]=0.0\n",
    "        X_df=X_df[feature_names]\n",
    "\n",
    "    return X_df, y\n",
    "\n",
    "# ---- load main first (defines feature_names/order) ----\n",
    "X_main_df, y_main = _read_preprocessed(MAIN_CSV, feature_names=None)\n",
    "feature_names = list(X_main_df.columns)\n",
    "num_features  = int(len(feature_names))\n",
    "X_main = X_main_df.to_numpy(dtype=np.float32)\n",
    "\n",
    "# ---- load live (forced to same feature_names/order) ----\n",
    "X_live_df, y_live = _read_preprocessed(LIVE_CSV, feature_names=feature_names)\n",
    "X_live = X_live_df.to_numpy(dtype=np.float32)\n",
    "\n",
    "# ---- desired class counts come from MAIN (keeps your original class mix) ----\n",
    "seed = int(seed) if \"seed\" in globals() else 1337\n",
    "rng  = np.random.default_rng(seed)\n",
    "\n",
    "idx0_main = np.where(y_main==0)[0]\n",
    "idx1_main = np.where(y_main==1)[0]\n",
    "idx0_live = np.where(y_live==0)[0]\n",
    "\n",
    "max_per_class = base_cfg.get(\"max_per_class\",None) if \"base_cfg\" in globals() else None\n",
    "B_des = len(idx0_main)\n",
    "M_des = len(idx1_main)\n",
    "if max_per_class is not None:\n",
    "    max_per_class=int(max_per_class)\n",
    "    B_des=min(B_des, max_per_class)\n",
    "    M_des=min(M_des, max_per_class)\n",
    "\n",
    "# split sizes (70/20/10), per-class so ratios stay consistent\n",
    "B_tr=int(B_des*0.70); B_va=int(B_des*0.20); B_te=B_des-B_tr-B_va\n",
    "M_tr=int(M_des*0.70); M_va=int(M_des*0.20); M_te=M_des-M_tr-M_va\n",
    "\n",
    "# \"50/50 benign\" per split (exact when even; closest possible when odd)\n",
    "Btr_live=B_tr//2; Bva_live=B_va//2; Bte_live=B_te//2\n",
    "Btr_main=B_tr-Btr_live; Bva_main=B_va-Bva_live; Bte_main=B_te-Bte_live\n",
    "B_live_need=Btr_live+Bva_live+Bte_live\n",
    "B_main_need=Btr_main+Bva_main+Bte_main\n",
    "\n",
    "# shuffle pools\n",
    "idx0_main = rng.permutation(idx0_main)[:B_main_need]\n",
    "idx1_main = rng.permutation(idx1_main)[:M_des]\n",
    "idx0_live_sh = rng.permutation(idx0_live)\n",
    "\n",
    "# if live benign is short, oversample WITH replacement to satisfy the 50/50 requirement\n",
    "if len(idx0_live_sh) >= B_live_need:\n",
    "    idx0_live_use = idx0_live_sh[:B_live_need]\n",
    "else:\n",
    "    if len(idx0_live_sh)==0:\n",
    "        raise SystemExit(\"Live CSV has 0 BENIGN rows; cannot make 50% benign-from-live splits.\")\n",
    "    need=B_live_need-len(idx0_live_sh)\n",
    "    idx0_live_use = np.concatenate([idx0_live_sh, rng.choice(idx0_live_sh, size=need, replace=True)])\n",
    "    print(f\"[warn] live benign short: have {len(idx0_live_sh)} need {B_live_need} -> oversampling {need} rows\")\n",
    "\n",
    "# slice into train/val/test\n",
    "p=0\n",
    "btr_live=idx0_live_use[p:p+Btr_live]; p+=Btr_live\n",
    "bva_live=idx0_live_use[p:p+Bva_live]; p+=Bva_live\n",
    "bte_live=idx0_live_use[p:p+Bte_live]; p+=Bte_live\n",
    "\n",
    "q=0\n",
    "btr_main=idx0_main[q:q+Btr_main]; q+=Btr_main\n",
    "bva_main=idx0_main[q:q+Bva_main]; q+=Bva_main\n",
    "bte_main=idx0_main[q:q+Bte_main]; q+=Bte_main\n",
    "\n",
    "r=0\n",
    "mtr=idx1_main[r:r+M_tr]; r+=M_tr\n",
    "mva=idx1_main[r:r+M_va]; r+=M_va\n",
    "mte=idx1_main[r:r+M_te]; r+=M_te\n",
    "\n",
    "# build splits (and shuffle within each split)\n",
    "def _mk_split(b_main_idx, b_live_idx, m_idx):\n",
    "    Xb_main = X_main[b_main_idx]\n",
    "    Xb_live = X_live[b_live_idx]\n",
    "    Xm      = X_main[m_idx]\n",
    "    Xs = np.concatenate([Xb_main, Xb_live, Xm], axis=0).astype(np.float32, copy=False)\n",
    "    ys = np.concatenate([np.zeros(len(Xb_main)+len(Xb_live),dtype=np.int64),\n",
    "                         np.ones(len(Xm),dtype=np.int64)], axis=0)\n",
    "    perm = rng.permutation(len(ys))\n",
    "    return Xs[perm], ys[perm]\n",
    "\n",
    "X_tr, y_tr = _mk_split(btr_main, btr_live, mtr)\n",
    "X_va, y_va = _mk_split(bva_main, bva_live, mva)\n",
    "X_te, y_te = _mk_split(bte_main, bte_live, mte)\n",
    "\n",
    "# ---- normalize (fit on train only) ----\n",
    "scaler=StandardScaler()\n",
    "X_tr=scaler.fit_transform(X_tr).astype(np.float32)\n",
    "X_va=scaler.transform(X_va).astype(np.float32)\n",
    "X_te=scaler.transform(X_te).astype(np.float32)\n",
    "\n",
    "scaler_mean=scaler.mean_.astype(np.float32).tolist()\n",
    "scaler_scale=scaler.scale_.astype(np.float32).tolist()\n",
    "\n",
    "# ---- torch datasets (Conv1d: (B,1,F)) ----\n",
    "X_tr=torch.from_numpy(X_tr).unsqueeze(1); y_tr=torch.from_numpy(y_tr).float()\n",
    "X_va=torch.from_numpy(X_va).unsqueeze(1); y_va=torch.from_numpy(y_va).float()\n",
    "X_te=torch.from_numpy(X_te).unsqueeze(1); y_te=torch.from_numpy(y_te).float()\n",
    "\n",
    "train_ds=TensorDataset(X_tr,y_tr); val_ds=TensorDataset(X_va,y_va); test_ds=TensorDataset(X_te,y_te)\n",
    "\n",
    "BATCH_SIZE=int(base_cfg.get(\"batch_size\",1000)) if \"base_cfg\" in globals() else 1000\n",
    "NUM_WORKERS=int(base_cfg.get(\"num_workers\",8)) if \"base_cfg\" in globals() else 8\n",
    "mp_ctx=torch.multiprocessing.get_context(\"spawn\")\n",
    "\n",
    "train_dl=DataLoader(train_ds,batch_size=BATCH_SIZE,shuffle=True ,num_workers=NUM_WORKERS,pin_memory=use_cuda,persistent_workers=True,multiprocessing_context=mp_ctx)\n",
    "val_dl  =DataLoader(val_ds  ,batch_size=BATCH_SIZE,shuffle=False,num_workers=NUM_WORKERS,pin_memory=use_cuda,persistent_workers=True,multiprocessing_context=mp_ctx)\n",
    "test_dl =DataLoader(test_ds ,batch_size=BATCH_SIZE,shuffle=False,num_workers=NUM_WORKERS,pin_memory=use_cuda,persistent_workers=True,multiprocessing_context=mp_ctx)\n",
    "\n",
    "pos_weight_value=float((y_tr.numel()-y_tr.sum()).clamp_min(1)/y_tr.sum().clamp_min(1))  # n_neg/n_pos\n",
    "\n",
    "print(\"X shape:\",tuple(X_tr.shape),\"num_features:\",num_features)\n",
    "print(\"splits:\",len(train_ds),len(val_ds),len(test_ds))\n",
    "print(\"benign/mal (MAIN original):\",int((y_main==0).sum()),int((y_main==1).sum()))\n",
    "print(\"benign live available:\",int((y_live==0).sum()),\"| live used total:\",B_live_need)\n",
    "print(\"benign live per split train/val/test:\",Btr_live,Bva_live,Bte_live)\n",
    "print(\"pos rate train/val/test:\",float(y_tr.mean()),float(y_va.mean()),float(y_te.mean()))\n",
    "print(\"pos_weight_value (n_neg/n_pos):\",pos_weight_value)\n",
    "print(\"feature_names:\",len(feature_names),\"| scaler_mean/scale:\",len(scaler_mean),len(scaler_scale))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "18676c3e-8061-4826-af76-249d28f7e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, csv, json, math, time\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve, auc, f1_score, accuracy_score, balanced_accuracy_score)\n",
    "\n",
    "# --------------------------\n",
    "# dirs / config I/O\n",
    "# --------------------------\n",
    "def prepare_run_dirs(results_root):\n",
    "    root=Path(results_root); root.mkdir(parents=True,exist_ok=True)\n",
    "    ids=[]\n",
    "    for p in root.iterdir():\n",
    "        if p.is_dir() and p.name.startswith(\"I\"):\n",
    "            try: ids.append(int(p.name.split(\"_\")[0][1:]))\n",
    "            except: pass\n",
    "    k=1\n",
    "    while k in ids: k+=1\n",
    "    run_dir=root/f\"I{k}\"\n",
    "    (run_dir/\"models\").mkdir(parents=True,exist_ok=True); (run_dir/\"final_metrics\").mkdir(parents=True,exist_ok=True)\n",
    "    return run_dir\n",
    "\n",
    "def make_grid_dir_name(run_dir,grid_idx,param_grid):\n",
    "    run_dir=Path(run_dir); kv=\"__\".join(f\"{k}=\"+\"|\".join(str(v) for v in vs) for k,vs in param_grid.items())\n",
    "    grid_dir=run_dir/f\"g{grid_idx}__{kv}\"\n",
    "    grid_dir.mkdir(parents=True,exist_ok=True)\n",
    "    return grid_dir\n",
    "\n",
    "def write_config_csv(exp_dir,cfg,epochs,extras=None):\n",
    "    exp_dir=Path(exp_dir); extras={} if extras is None else dict(extras)\n",
    "    row=dict(cfg); row.update({\"epochs\":int(epochs)}); row.update(extras)\n",
    "    path=exp_dir/\"config.csv\"\n",
    "    with path.open(\"w\",newline=\"\") as f:\n",
    "        w=csv.DictWriter(f,fieldnames=list(row.keys())); w.writeheader(); w.writerow(row)\n",
    "\n",
    "def _save_csv(path,rows):\n",
    "    path=Path(path); path.parent.mkdir(parents=True,exist_ok=True)\n",
    "    if not rows: return\n",
    "    cols=list(rows[0].keys())\n",
    "    with path.open(\"w\",newline=\"\") as f:\n",
    "        w=csv.DictWriter(f,fieldnames=cols); w.writeheader(); [w.writerow(r) for r in rows]\n",
    "\n",
    "def _df_print(rows,sort_by=None,head=None,title=None,round_=4):\n",
    "    if not rows: print(\"(no rows)\"); return\n",
    "    df=pd.DataFrame(rows)\n",
    "    if sort_by is not None and sort_by in df.columns: df=df.sort_values(sort_by,ascending=False)\n",
    "    if head is not None: df=df.head(int(head))\n",
    "    if round_ is not None:\n",
    "        for c in df.columns:\n",
    "            if pd.api.types.is_float_dtype(df[c]): df[c]=df[c].round(round_)\n",
    "    if title: print(f\"\\n{title}\")\n",
    "    print(df.to_string(index=False))\n",
    "\n",
    "# --------------------------\n",
    "# data cache + loader rebuild (supports grid over batch_size/max_per_class)\n",
    "# also caches feature order + scaler stats for live inference export\n",
    "# --------------------------\n",
    "_DATA_CACHE=None\n",
    "\n",
    "def init_data_cache_from_existing(train_ds=None,val_ds=None,test_ds=None,feature_names=None,scaler=None,scaler_mean=None,scaler_scale=None):\n",
    "    global _DATA_CACHE\n",
    "    if _DATA_CACHE is not None: return _DATA_CACHE\n",
    "    if train_ds is None: train_ds=globals().get(\"train_ds\",None)\n",
    "    if val_ds   is None: val_ds  =globals().get(\"val_ds\"  ,None)\n",
    "    if test_ds  is None: test_ds =globals().get(\"test_ds\" ,None)\n",
    "    if train_ds is None or val_ds is None or test_ds is None: return None\n",
    "\n",
    "    X=torch.cat([train_ds.tensors[0],val_ds.tensors[0],test_ds.tensors[0]],0).cpu().numpy().astype(np.float32)  # (N,1,F)\n",
    "    y0=torch.cat([train_ds.tensors[1],val_ds.tensors[1],test_ds.tensors[1]],0).cpu().numpy()\n",
    "    y_int=((y0>0.5).astype(np.int64) if y0.dtype!=np.int64 else y0.astype(np.int64))\n",
    "    y_f=y_int.astype(np.float32)\n",
    "\n",
    "    n_tr,n_va,n_te=len(train_ds),len(val_ds),len(test_ds)\n",
    "\n",
    "    if feature_names is None: feature_names=globals().get(\"feature_names\",None)\n",
    "    if scaler is None: scaler=globals().get(\"scaler\",None)\n",
    "    if scaler_mean is None: scaler_mean=globals().get(\"scaler_mean\",None)\n",
    "    if scaler_scale is None: scaler_scale=globals().get(\"scaler_scale\",None)\n",
    "    if scaler is not None:\n",
    "        if scaler_mean is None and hasattr(scaler,\"mean_\"):  scaler_mean=scaler.mean_\n",
    "        if scaler_scale is None and hasattr(scaler,\"scale_\"): scaler_scale=scaler.scale_\n",
    "\n",
    "    _DATA_CACHE={\n",
    "        \"X\":X, \"y_int\":y_int, \"y_f\":y_f,\n",
    "        \"n_tr\":n_tr, \"n_va\":n_va, \"n_te\":n_te,\n",
    "        \"num_features\":int(X.shape[-1]),\n",
    "        \"feature_names\":(list(feature_names) if feature_names is not None else None),\n",
    "        \"scaler_mean\":(np.asarray(scaler_mean,dtype=np.float32).tolist() if scaler_mean is not None else None),\n",
    "        \"scaler_scale\":(np.asarray(scaler_scale,dtype=np.float32).tolist() if scaler_scale is not None else None),\n",
    "    }\n",
    "    return _DATA_CACHE\n",
    "\n",
    "def get_ckpt_preproc_fields():\n",
    "    c=init_data_cache_from_existing()\n",
    "    if c is None: return {}\n",
    "    out={}\n",
    "    if c.get(\"feature_names\") is not None: out[\"feature_names\"]=c[\"feature_names\"]\n",
    "    if c.get(\"scaler_mean\")  is not None: out[\"scaler_mean\"]=c[\"scaler_mean\"]\n",
    "    if c.get(\"scaler_scale\") is not None: out[\"scaler_scale\"]=c[\"scaler_scale\"]\n",
    "    return out\n",
    "\n",
    "def make_loaders(cfg,seed=1337,use_cuda=None):\n",
    "    cache=init_data_cache_from_existing()\n",
    "    bs=int(cfg.get(\"batch_size\",1000)); nw=int(cfg.get(\"num_workers\",8))\n",
    "    use_cuda=torch.cuda.is_available() if use_cuda is None else bool(use_cuda)\n",
    "    mp_ctx=torch.multiprocessing.get_context(\"spawn\")\n",
    "\n",
    "    # fallback: just rewrap existing datasets with new bs/nw\n",
    "    if cache is None:\n",
    "        td,vd,sd=globals().get(\"train_ds\"),globals().get(\"val_ds\"),globals().get(\"test_ds\")\n",
    "        loss=str(cfg.get(\"loss\",\"ce\")).lower()\n",
    "        ysum=td.tensors[1].float().sum()\n",
    "        pos_weight=float((td.tensors[1].numel()-ysum).clamp_min(1)/ysum.clamp_min(1))\n",
    "        return (DataLoader(td,batch_size=bs,shuffle=True ,num_workers=nw,pin_memory=use_cuda,persistent_workers=True,multiprocessing_context=mp_ctx),\n",
    "                DataLoader(vd,batch_size=bs,shuffle=False,num_workers=nw,pin_memory=use_cuda,persistent_workers=True,multiprocessing_context=mp_ctx),\n",
    "                DataLoader(sd,batch_size=bs,shuffle=False,num_workers=nw,pin_memory=use_cuda,persistent_workers=True,multiprocessing_context=mp_ctx),\n",
    "                int(td.tensors[0].shape[-1]), pos_weight)\n",
    "\n",
    "    X=cache[\"X\"]; y_int=cache[\"y_int\"]; y_f=cache[\"y_f\"]\n",
    "    n_tr,n_va,n_te=cache[\"n_tr\"],cache[\"n_va\"],cache[\"n_te\"]\n",
    "    X_tr,X_va,X_te=X[:n_tr],X[n_tr:n_tr+n_va],X[n_tr+n_va:n_tr+n_va+n_te]\n",
    "    ytr_i,yva_i,yte_i=y_int[:n_tr],y_int[n_tr:n_tr+n_va],y_int[n_tr+n_va:n_tr+n_va+n_te]\n",
    "    ytr_f,yva_f,yte_f=y_f[:n_tr],y_f[n_tr:n_tr+n_va],y_f[n_tr+n_va:n_tr+n_va+n_te]\n",
    "\n",
    "    # optional: cap training set per class (deterministic)\n",
    "    mpc=cfg.get(\"max_per_class\",None)\n",
    "    if mpc is not None:\n",
    "        rng=np.random.default_rng(int(cfg.get(\"seed\",seed)))\n",
    "        mpc=int(mpc)\n",
    "        idx0=np.where(ytr_i==0)[0]; idx1=np.where(ytr_i==1)[0]\n",
    "        if len(idx0)>mpc: idx0=rng.choice(idx0,size=mpc,replace=False)\n",
    "        if len(idx1)>mpc: idx1=rng.choice(idx1,size=mpc,replace=False)\n",
    "        idx=np.concatenate([idx0,idx1]); rng.shuffle(idx)\n",
    "        X_tr,ytr_i,ytr_f=X_tr[idx],ytr_i[idx],ytr_f[idx]\n",
    "\n",
    "    loss=str(cfg.get(\"loss\",\"ce\")).lower()\n",
    "    y_tr=torch.from_numpy(ytr_i) if loss==\"ce\" else torch.from_numpy(ytr_f)\n",
    "    y_va=torch.from_numpy(yva_i) if loss==\"ce\" else torch.from_numpy(yva_f)\n",
    "    y_te=torch.from_numpy(yte_i) if loss==\"ce\" else torch.from_numpy(yte_f)\n",
    "    X_tr=torch.from_numpy(X_tr); X_va=torch.from_numpy(X_va); X_te=torch.from_numpy(X_te)\n",
    "\n",
    "    train_ds=TensorDataset(X_tr,y_tr); val_ds=TensorDataset(X_va,y_va); test_ds=TensorDataset(X_te,y_te)\n",
    "    train_dl=DataLoader(train_ds,batch_size=bs,shuffle=True ,num_workers=nw,pin_memory=use_cuda,persistent_workers=True,multiprocessing_context=mp_ctx)\n",
    "    val_dl  =DataLoader(val_ds  ,batch_size=bs,shuffle=False,num_workers=nw,pin_memory=use_cuda,persistent_workers=True,multiprocessing_context=mp_ctx)\n",
    "    test_dl =DataLoader(test_ds ,batch_size=bs,shuffle=False,num_workers=nw,pin_memory=use_cuda,persistent_workers=True,multiprocessing_context=mp_ctx)\n",
    "\n",
    "    pos_weight=float((len(y_tr)-float(y_tr.float().sum()))/max(float(y_tr.float().sum()),1.0))\n",
    "    return train_dl,val_dl,test_dl,int(cache[\"num_features\"]),pos_weight\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d791ad6c-7c37-456f-9696-f1cb664f423b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cfg = {\n",
    "    # data / loader\n",
    "    \"batch_size\": 1000,\n",
    "    \"num_workers\": 8,\n",
    "    \"max_per_class\": None,\n",
    "\n",
    "    # model (3 conv blocks + 2 FC)\n",
    "    \"conv_channels\": (32, 64, 128),\n",
    "    \"kernel_sizes\": (5, 5, 3),\n",
    "    \"pools\": (2, 2, 2),\n",
    "    \"fc_hidden\": (256, 64),\n",
    "    \"conv_dropout\": (0.05, 0.05, 0.10),   # fixed unless you later decide to sweep\n",
    "    \"fc_dropout\": (0.50, 0.50),           # fixed unless you later decide to sweep\n",
    "    \"negative_slope\": 0.01,\n",
    "    \"use_bn\": False,\n",
    "\n",
    "    # optimization\n",
    "    \"loss\": \"ce\",                         # default\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"lr\": 1e-2,\n",
    "    \"weight_decay\": 0.0,\n",
    "\n",
    "    # scheduler (off by default; cfg enables it by setting gamma>0)\n",
    "    \"sched_gamma\": 0.0,\n",
    "    \"sched_step_size\": 10,\n",
    "\n",
    "    # training extras (optional; keep 0 to disable)\n",
    "    \"patience\": 0,\n",
    "    \"grad_clip\": 1.0,\n",
    "    \"amp\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8464bc3-7047-4dc2-8947-f8e93e37ca9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_dir: /home/cis6022/Adam/varMax/results/I5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3361503/2445664648.py:264: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  amp=bool(cfg.get(\"amp\",False)) and use_cuda; scaler=torch.cuda.amp.GradScaler(enabled=amp)\n",
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep001 tr_loss=0.0487 val_loss=0.0109 val_bestF1=0.9987 val_thr=0.4383 val_roc=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep002 tr_loss=0.0350 val_loss=0.0094 val_bestF1=0.9985 val_thr=0.5771 val_roc=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep003 tr_loss=0.0238 val_loss=0.0094 val_bestF1=0.9990 val_thr=0.6159 val_roc=0.9998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep004 tr_loss=0.0449 val_loss=0.0279 val_bestF1=0.9988 val_thr=0.5845 val_roc=0.9998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep005 tr_loss=0.0887 val_loss=0.0091 val_bestF1=0.9989 val_thr=0.8103 val_roc=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep006 tr_loss=0.0169 val_loss=0.0064 val_bestF1=0.9991 val_thr=0.6551 val_roc=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep007 tr_loss=0.1101 val_loss=0.0138 val_bestF1=0.9988 val_thr=0.6100 val_roc=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep008 tr_loss=0.0132 val_loss=0.0065 val_bestF1=0.9992 val_thr=0.5045 val_roc=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep009 tr_loss=28.7291 val_loss=0.0057 val_bestF1=0.9992 val_thr=0.8604 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep010 tr_loss=0.8543 val_loss=0.0064 val_bestF1=0.9992 val_thr=0.8189 val_roc=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep011 tr_loss=0.0183 val_loss=0.0129 val_bestF1=0.9991 val_thr=0.5109 val_roc=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep012 tr_loss=4.7544 val_loss=0.0085 val_bestF1=0.9990 val_thr=0.5254 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep013 tr_loss=1.4999 val_loss=0.0078 val_bestF1=0.9992 val_thr=0.8511 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep014 tr_loss=0.0304 val_loss=0.0074 val_bestF1=0.9992 val_thr=0.7272 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep015 tr_loss=1.5762 val_loss=0.0074 val_bestF1=0.9991 val_thr=0.8742 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep016 tr_loss=2.6909 val_loss=0.0159 val_bestF1=0.9985 val_thr=0.9485 val_roc=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep017 tr_loss=0.0195 val_loss=0.0063 val_bestF1=0.9992 val_thr=0.5115 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep018 tr_loss=0.4708 val_loss=0.0064 val_bestF1=0.9994 val_thr=0.6452 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep019 tr_loss=0.0738 val_loss=0.0098 val_bestF1=0.9993 val_thr=0.7903 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep020 tr_loss=0.0613 val_loss=15.0829 val_bestF1=0.9975 val_thr=0.7081 val_roc=0.9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep021 tr_loss=0.1714 val_loss=0.0061 val_bestF1=0.9993 val_thr=0.8821 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep022 tr_loss=0.0098 val_loss=0.0053 val_bestF1=0.9994 val_thr=0.6476 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep023 tr_loss=0.0742 val_loss=0.0062 val_bestF1=0.9993 val_thr=0.8894 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep024 tr_loss=0.0163 val_loss=0.0095 val_bestF1=0.9993 val_thr=0.2576 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep025 tr_loss=0.1652 val_loss=0.0084 val_bestF1=0.9991 val_thr=0.7694 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep026 tr_loss=0.8278 val_loss=0.0068 val_bestF1=0.9993 val_thr=0.9050 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep027 tr_loss=0.0356 val_loss=0.0067 val_bestF1=0.9993 val_thr=0.8755 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep028 tr_loss=0.0465 val_loss=0.0054 val_bestF1=0.9994 val_thr=0.8442 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep029 tr_loss=0.0191 val_loss=0.0125 val_bestF1=0.9993 val_thr=0.7427 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep030 tr_loss=0.0161 val_loss=0.0081 val_bestF1=0.9993 val_thr=0.8268 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep031 tr_loss=0.0630 val_loss=0.0117 val_bestF1=0.9987 val_thr=0.8309 val_roc=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep032 tr_loss=0.1369 val_loss=0.0060 val_bestF1=0.9993 val_thr=0.3324 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep033 tr_loss=0.1841 val_loss=0.0073 val_bestF1=0.9993 val_thr=0.8930 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep034 tr_loss=0.1597 val_loss=0.0123 val_bestF1=0.9991 val_thr=0.6854 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep035 tr_loss=0.0132 val_loss=0.0078 val_bestF1=0.9992 val_thr=0.8877 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep036 tr_loss=0.2439 val_loss=0.0144 val_bestF1=0.9991 val_thr=0.9562 val_roc=0.9998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep037 tr_loss=0.0820 val_loss=0.0071 val_bestF1=0.9994 val_thr=0.6256 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep038 tr_loss=0.4443 val_loss=0.0090 val_bestF1=0.9993 val_thr=0.9517 val_roc=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep039 tr_loss=0.1064 val_loss=0.0085 val_bestF1=0.9993 val_thr=0.8412 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep040 tr_loss=0.0419 val_loss=0.0100 val_bestF1=0.9994 val_thr=0.4935 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep041 tr_loss=2.1428 val_loss=0.0072 val_bestF1=0.9993 val_thr=0.4291 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep042 tr_loss=0.1015 val_loss=0.0053 val_bestF1=0.9995 val_thr=0.7564 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep043 tr_loss=0.1051 val_loss=0.0080 val_bestF1=0.9993 val_thr=0.8534 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep044 tr_loss=0.0625 val_loss=0.0103 val_bestF1=0.9994 val_thr=0.8798 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep045 tr_loss=0.0125 val_loss=0.0095 val_bestF1=0.9994 val_thr=0.9728 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep046 tr_loss=0.0097 val_loss=0.0116 val_bestF1=0.9995 val_thr=0.4969 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep047 tr_loss=2.5676 val_loss=0.0064 val_bestF1=0.9994 val_thr=0.8376 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep048 tr_loss=0.0109 val_loss=0.0090 val_bestF1=0.9994 val_thr=0.1497 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep049 tr_loss=0.9176 val_loss=0.0097 val_bestF1=0.9994 val_thr=0.8714 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep050 tr_loss=0.0882 val_loss=0.0074 val_bestF1=0.9994 val_thr=0.5199 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep051 tr_loss=0.0256 val_loss=0.0188 val_bestF1=0.9995 val_thr=0.3745 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep052 tr_loss=0.0744 val_loss=0.0072 val_bestF1=0.9994 val_thr=0.5923 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep053 tr_loss=0.0340 val_loss=0.0135 val_bestF1=0.9996 val_thr=0.1859 val_roc=0.9999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep054 tr_loss=0.0454 val_loss=0.0054 val_bestF1=0.9996 val_thr=0.6267 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep055 tr_loss=0.0196 val_loss=0.0067 val_bestF1=0.9993 val_thr=0.6541 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep056 tr_loss=0.0269 val_loss=0.0068 val_bestF1=0.9994 val_thr=0.7139 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep057 tr_loss=0.0294 val_loss=0.0040 val_bestF1=0.9995 val_thr=0.7829 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep058 tr_loss=0.0415 val_loss=0.0041 val_bestF1=0.9996 val_thr=0.1952 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep059 tr_loss=0.1519 val_loss=0.0064 val_bestF1=0.9994 val_thr=0.6740 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep060 tr_loss=0.3611 val_loss=0.0038 val_bestF1=0.9996 val_thr=0.7085 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep061 tr_loss=2.5752 val_loss=0.0067 val_bestF1=0.9994 val_thr=0.8635 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep062 tr_loss=0.0532 val_loss=0.0042 val_bestF1=0.9996 val_thr=0.7125 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep063 tr_loss=0.0591 val_loss=0.0070 val_bestF1=0.9994 val_thr=0.6633 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep064 tr_loss=0.5140 val_loss=0.0099 val_bestF1=0.9993 val_thr=0.9036 val_roc=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–‹         | 64/988 [00:00<00:08, 111.53it/s]"
     ]
    }
   ],
   "source": [
    "RESULTS_ROOT=os.path.join(ROOT,\"results\"); run_dir=prepare_run_dirs(RESULTS_ROOT); print(\"run_dir:\",run_dir)\n",
    "\n",
    "EPOCHS=100\n",
    "base_cfg={\n",
    "    \"batch_size\":1000,\"num_workers\":8,\"max_per_class\":None,\n",
    "    \"conv_channels\":(32,64,128),\"kernel_sizes\":(5,5,3),\"pools\":(2,2,2),\n",
    "    \"fc_hidden\":(256,64),\"conv_dropout\":(0.05,0.05,0.10),\"fc_dropout\":(0.50,0.50),\n",
    "    \"negative_slope\":0.01,\"use_bn\":False,\n",
    "    \"loss\":\"ce\",\"optimizer\":\"Adam\",\"lr\":1e-2,\"weight_decay\":0.0,\n",
    "    \"sched_gamma\":0.0,\"sched_step_size\":10,\n",
    "    \"patience\":0,\"grad_clip\":1.0,\"amp\":False,\n",
    "}\n",
    "\n",
    "grid_specs=[\n",
    "    {\"params\":{\"lr\":[1e-1,3e-2,1e-2,3e-3,1e-3],\"weight_decay\":[0,1e-6,1e-5,1e-4,1e-3]},\"epochs\":25},\n",
    "    {\"params\":{\"sched_gamma\":[0.8,0.9],\"sched_step_size\":[5,10,20]},\"epochs\":25},\n",
    "    {\"params\":{\"batch_size\":[256,512,1000,2048],\"lr\":[1e-1,3e-2,1e-2,3e-3,1e-3]},\"epochs\":25},\n",
    "    {\"params\":{\"conv_channels\":[(16,32,64),(32,64,128),(64,128,256)],\"fc_hidden\":[(128,64),(256,64),(512,128)]},\"epochs\":25},\n",
    "    {\"params\":{\"kernel_sizes\":[(3,3,3),(5,5,3),(7,5,3)],\"pools\":[(2,2,2),(4,2,2)]},\"epochs\":25},\n",
    "    {\"params\":{\"max_per_class\":[None,300_000,100_000],\"lr\":[1e-1,3e-2,1e-2,3e-3,1e-3]},\"epochs\":25},\n",
    "    {\"params\":{\"loss\":[\"ce\",\"bce\"],\"lr\":[1e-2,3e-3,1e-3]},\"epochs\":25},\n",
    "]\n",
    "\n",
    "exp_dir=Path(run_dir)/f\"final_{EPOCHS}ep\"; (exp_dir/\"models\").mkdir(parents=True,exist_ok=True); (exp_dir/\"final_metrics\").mkdir(parents=True,exist_ok=True)\n",
    "write_config_csv(exp_dir,base_cfg,EPOCHS,extras={\"num_features\":int(num_features)})\n",
    "\n",
    "history,best_val_f1,metrics_test=run_experiment(base_cfg,EPOCHS,exp_dir,seed=base_cfg.get(\"seed\",1337),score_key=\"val_best_f1\")\n",
    "print(\"best_val_macro_f1:\",best_val_f1); print(\"test_metrics:\",metrics_test)\n",
    "\n",
    "# Uncomment to run grids (this can be a lot of runs):\n",
    "# all_rows=run_grid_searches(run_dir,base_cfg,grid_specs,epochs_default=EPOCHS,seed=base_cfg.get(\"seed\",1337))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f6fe072-1aa2-4f4e-b380-d08f535edd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re-saved: cicids_cnn_best_ckpt.pt | keys: ['model_state', 'cfg', 'epoch', 'val_best_thr', 'feature_names', 'scaler_mean', 'scaler_scale']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "SRC=Path(\"/home/cis6022/Adam/varMax/results/I2/final_100ep/models/best.pt\")\n",
    "DST=Path(\"cicids_cnn_best_ckpt.pt\")  # write a new portable ckpt next to your notebook\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ckpt=torch.load(SRC,map_location=\"cpu\")  # save in cpu format for portability\n",
    "\n",
    "# pull from your notebook globals (these should exist from your dataloader cell)\n",
    "if \"feature_names\" in globals(): ckpt[\"feature_names\"]=list(feature_names)\n",
    "if \"scaler\" in globals():\n",
    "    ckpt[\"scaler_mean\"]=scaler.mean_.astype(\"float32\")\n",
    "    ckpt[\"scaler_scale\"]=scaler.scale_.astype(\"float32\")\n",
    "if \"scaler_mean\" in globals(): ckpt[\"scaler_mean\"]=np.asarray(scaler_mean,dtype=\"float32\")\n",
    "if \"scaler_scale\" in globals(): ckpt[\"scaler_scale\"]=np.asarray(scaler_scale,dtype=\"float32\")\n",
    "\n",
    "torch.save(ckpt,DST)\n",
    "print(\"re-saved:\",DST,\"| keys:\",list(ckpt.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "49e1dfa1-29de-4318-87be-e593894749f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /home/cis6022/Adam/varMax/CICIDS_inject_malicious_pool_10k.csv rows: 10000 cols: 1504\n",
      "head cols: ['payload_byte_1', 'payload_byte_2', 'payload_byte_3', 'payload_byte_4', 'payload_byte_5', 'payload_byte_6', 'payload_byte_7', 'payload_byte_8']  ... tail cols: ['ttl', 'total_len', 'proto_bin', 't_delta']\n"
     ]
    }
   ],
   "source": [
    "# --- Cell: build an \"injectable\" malicious packet pool (10k rows) in LIVE format ---\n",
    "\n",
    "import numpy as np, pandas as pd, torch\n",
    "from pathlib import Path\n",
    "\n",
    "CKPT_PATH=\"cicids_cnn_best_ckpt.pt\"\n",
    "SOURCE_CSV=\"# --- Cell: build an \"injectable\" malicious packet pool (10k rows) in LIVE format ---\n",
    "\n",
    "import numpy as np, pandas as pd, torch\n",
    "from pathlib import Path\n",
    "\n",
    "CKPT_PATH=\"cicids_cnn_best_ckpt.pt\"\n",
    "SOURCE_CSV=\"CICIDS-2017_preprocessed.csv\"          # <-- your \"other dataset\" (must contain malicious rows)\n",
    "OUT_CSV=\"CICIDS_inject_malicious_pool_10k.csv\"\n",
    "N=10_000\n",
    "SEED=1337\n",
    "\n",
    "def _torch_load(path,device):\n",
    "    try: return torch.load(path,map_location=device,weights_only=False)\n",
    "    except TypeError: return torch.load(path,map_location=device)\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ckpt=_torch_load(CKPT_PATH,device)\n",
    "feature_names=ckpt.get(\"feature_names\",None)\n",
    "if feature_names is None: raise SystemExit(f\"ckpt missing feature_names. Keys: {list(ckpt.keys())}\")\n",
    "\n",
    "# expect LIVE columns: payload_byte_1..1500 + ttl,total_len,proto_bin,t_delta\n",
    "PAYLOAD_COLS=[f\"payload_byte_{i+1}\" for i in range(1500)]\n",
    "ALL_COLS=PAYLOAD_COLS+[\"ttl\",\"total_len\",\"proto_bin\",\"t_delta\"]\n",
    "\n",
    "src_path=(Path(SOURCE_CSV) if Path(SOURCE_CSV).exists()\n",
    "          else (DATA_DIR/SOURCE_CSV if \"DATA_DIR\" in globals() and (DATA_DIR/SOURCE_CSV).exists() else Path(ROOT)/SOURCE_CSV))\n",
    "if not src_path.exists(): raise SystemExit(f\"missing SOURCE_CSV: {src_path}\")\n",
    "\n",
    "# load only what we need (fast) + label column if present\n",
    "hdr=pd.read_csv(src_path,nrows=0)\n",
    "cols=set(hdr.columns)\n",
    "label_col=\"Label\" if \"Label\" in cols else (\"label\" if \"label\" in cols else None)\n",
    "usecols=[c for c in (set(ALL_COLS)|set(feature_names)) if c in cols] + ([label_col] if label_col else [])\n",
    "df=pd.read_csv(src_path,usecols=usecols)\n",
    "\n",
    "if label_col and label_col!=\"Label\": df=df.rename(columns={label_col:\"Label\"}); label_col=\"Label\"\n",
    "if not label_col: raise SystemExit(f\"{src_path.name} has no Label/label column; cannot filter malicious rows.\")\n",
    "\n",
    "df=df.replace([np.inf,-np.inf],np.nan).dropna()\n",
    "mal=df[df[\"Label\"].astype(str).str.strip().str.upper()!=\"BENIGN\"].copy()\n",
    "if len(mal)==0: raise SystemExit(\"No malicious rows found (Label != BENIGN).\")\n",
    "\n",
    "rng=np.random.default_rng(SEED)\n",
    "idx=rng.choice(len(mal),size=N,replace=(len(mal)<N))\n",
    "mal=mal.iloc[idx].copy()\n",
    "\n",
    "# enforce EXACT LIVE schema + ordering (matches live_detection.py DataFrame)\n",
    "d=mal.drop(columns=[c for c in (\"Flow ID\",\"Source IP\",\"Source Port\",\"Destination IP\",\"Destination Port\",\"Timestamp\",\"Label\",\"label\") if c in mal.columns],errors=\"ignore\")\n",
    "for c in ALL_COLS:\n",
    "    if c not in d.columns: d[c]=0.0\n",
    "d=d[ALL_COLS].replace([np.inf,-np.inf],np.nan).fillna(0.0)\n",
    "\n",
    "out_path=Path(OUT_CSV)\n",
    "d.to_csv(out_path,index=False)\n",
    "print(\"saved:\",out_path.resolve(),\"rows:\",len(d),\"cols:\",len(d.columns))\n",
    "print(\"head cols:\",d.columns[:8].tolist(),\" ... tail cols:\",d.columns[-4:].tolist())         # <-- your \"other dataset\" (must contain malicious rows)\n",
    "OUT_CSV=\"CICIDS_inject_malicious_pool_10k.csv\"\n",
    "N=10_000\n",
    "SEED=1337\n",
    "\n",
    "def _torch_load(path,device):\n",
    "    try: return torch.load(path,map_location=device,weights_only=False)\n",
    "    except TypeError: return torch.load(path,map_location=device)\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ckpt=_torch_load(CKPT_PATH,device)\n",
    "feature_names=ckpt.get(\"feature_names\",None)\n",
    "if feature_names is None: raise SystemExit(f\"ckpt missing feature_names. Keys: {list(ckpt.keys())}\")\n",
    "\n",
    "# expect LIVE columns: payload_byte_1..1500 + ttl,total_len,proto_bin,t_delta\n",
    "PAYLOAD_COLS=[f\"payload_byte_{i+1}\" for i in range(1500)]\n",
    "ALL_COLS=PAYLOAD_COLS+[\"ttl\",\"total_len\",\"proto_bin\",\"t_delta\"]\n",
    "\n",
    "src_path=(Path(SOURCE_CSV) if Path(SOURCE_CSV).exists()\n",
    "          else (DATA_DIR/SOURCE_CSV if \"DATA_DIR\" in globals() and (DATA_DIR/SOURCE_CSV).exists() else Path(ROOT)/SOURCE_CSV))\n",
    "if not src_path.exists(): raise SystemExit(f\"missing SOURCE_CSV: {src_path}\")\n",
    "\n",
    "# load only what we need (fast) + label column if present\n",
    "hdr=pd.read_csv(src_path,nrows=0)\n",
    "cols=set(hdr.columns)\n",
    "label_col=\"Label\" if \"Label\" in cols else (\"label\" if \"label\" in cols else None)\n",
    "usecols=[c for c in (set(ALL_COLS)|set(feature_names)) if c in cols] + ([label_col] if label_col else [])\n",
    "df=pd.read_csv(src_path,usecols=usecols)\n",
    "\n",
    "if label_col and label_col!=\"Label\": df=df.rename(columns={label_col:\"Label\"}); label_col=\"Label\"\n",
    "if not label_col: raise SystemExit(f\"{src_path.name} has no Label/label column; cannot filter malicious rows.\")\n",
    "\n",
    "df=df.replace([np.inf,-np.inf],np.nan).dropna()\n",
    "mal=df[df[\"Label\"].astype(str).str.strip().str.upper()!=\"BENIGN\"].copy()\n",
    "if len(mal)==0: raise SystemExit(\"No malicious rows found (Label != BENIGN).\")\n",
    "\n",
    "rng=np.random.default_rng(SEED)\n",
    "idx=rng.choice(len(mal),size=N,replace=(len(mal)<N))\n",
    "mal=mal.iloc[idx].copy()\n",
    "\n",
    "# enforce EXACT LIVE schema + ordering (matches live_detection.py DataFrame)\n",
    "d=mal.drop(columns=[c for c in (\"Flow ID\",\"Source IP\",\"Source Port\",\"Destination IP\",\"Destination Port\",\"Timestamp\",\"Label\",\"label\") if c in mal.columns],errors=\"ignore\")\n",
    "for c in ALL_COLS:\n",
    "    if c not in d.columns: d[c]=0.0\n",
    "d=d[ALL_COLS].replace([np.inf,-np.inf],np.nan).fillna(0.0)\n",
    "\n",
    "out_path=Path(OUT_CSV)\n",
    "d.to_csv(out_path,index=False)\n",
    "print(\"saved:\",out_path.resolve(),\"rows:\",len(d),\"cols:\",len(d.columns))\n",
    "print(\"head cols:\",d.columns[:8].tolist(),\" ... tail cols:\",d.columns[-4:].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7caf7b8a-78dc-435a-959b-84e14c9f5dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(ALL_COLS): 1504\n",
      "len(feature_names): 1504\n",
      "feature_names == ALL_COLS ? False\n",
      "model expects (not in ALL_COLS): ['protocol'] count: 1\n",
      "live has (not in model): ['proto_bin'] count: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"len(ALL_COLS):\", len(ALL_COLS))\n",
    "print(\"len(feature_names):\", None if feature_names is None else len(feature_names))\n",
    "print(\"feature_names == ALL_COLS ?\", feature_names == ALL_COLS)\n",
    "\n",
    "if feature_names is not None:\n",
    "    missing = [c for c in feature_names if c not in ALL_COLS]\n",
    "    extra   = [c for c in ALL_COLS if c not in feature_names]\n",
    "    print(\"model expects (not in ALL_COLS):\", missing[:20], \"count:\", len(missing))\n",
    "    print(\"live has (not in model):\", extra[:20], \"count:\", len(extra))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
